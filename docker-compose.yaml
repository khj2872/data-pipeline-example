version: '3.7'

# ====================================== AIRFLOW ENVIRONMENT VARIABLES =======================================
x-environment: &airflow_environment
  - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
  - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  - AIRFLOW__CORE__FERNET_KEY=hCRoPUYBO27QiEg1MRu5hSjLG7yNd8y8XKlm-8kRlkQ=
  - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__CORE__LOAD_EXAMPLES=False
  - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow
  - AIRFLOW__CORE__STORE_DAG_CODE=True
  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  - AIRFLOW_CONN_S3=s3://@?host=http://minio:9000&aws_access_key_id=AKIAIOSFODNN7EXAMPLE&aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

x-airflow-image: &airflow_image apache/airflow:2.2.5-python3.8
# ====================================== /AIRFLOW ENVIRONMENT VARIABLES ======================================

services:
  postgres:
    image: postgres:12-alpine
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  initdb_adduser:
    build:
      context: ./services/airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    depends_on:
      - postgres
    environment: *airflow_environment
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org'

  webserver:
    build:
      context: ./services/airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    restart: always
    depends_on:
      - postgres
    volumes:
      - ./dags:/opt/airflow/dags
      - ./spark/app:/usr/local/spark/app
      - logs:/opt/airflow/logs
    ports:
      - "18080:8080"
    environment: *airflow_environment
    command: webserver

  add_connection:
    build:
      context: ./services/airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    depends_on:
      - webserver
    environment: *airflow_environment
    entrypoint: /bin/bash
    command: -c 'airflow connections add --conn-host spark://spark-master --conn-port 7077 --conn-type Spark spark_local'

  scheduler:
    build:
      context: ./services/airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    restart: always
    depends_on:
      - postgres
    volumes:
      - ./dags:/opt/airflow/dags
      - ./spark/app:/usr/local/spark/app
      - logs:/opt/airflow/logs
    environment: *airflow_environment
    entrypoint: ["/bin/sh"]
    command: ["-c", "airflow scheduler"]

  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=datalake
    env_file:
      - ./hadoop.env

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    depends_on:
      - hadoop-namenode
    volumes:
      - ./data/datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  hadoop-datanode-init:
    build:
      context: ./services/hadoop
    depends_on:
      - hadoop-datanode
    env_file:
      - ./hadoop.env

  spark-master:
    image: bde2020/spark-master:3.2.0-hadoop3.2
#    container_name: spark-master
    ports:
      - "8081:8080"
      - "7077:7077"
    env_file:
      - ./hadoop.env

  spark-worker:
    image: bde2020/spark-worker:3.2.0-hadoop3.2
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    env_file:
      - ./hadoop.env

  zeppelin:
    build:
      context: ./services/zeppelin
    ports:
      - "8082:8080"
    volumes:
      - ./notebook:/opt/zeppelin/notebook
    depends_on:
      - spark
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_HOME=/spark

volumes:
  logs:
  s3: